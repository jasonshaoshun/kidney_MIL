{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CAMEL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP9dGxJ7PYLJ"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiLr2LpHQzFk"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Gf2VB8CatH"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import auc as auc_area_calc\n",
        "from torchvision import datasets, transforms, models\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve\n",
        "import copy\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "import tensorflow as tf\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW59hvGOnW2G"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYAevCwZnZkL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "import torch.utils.data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import copy\n",
        "from torchvision import models, transforms\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUeu0M__nc6L"
      },
      "source": [
        "# !pip install googledrivedownloader\n",
        "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "# gdd.download_file_from_google_drive(file_id='13vFqjgBqTWLuBwAVSOmZ8HotckfIZjor',\n",
        "#                                     dest_path='/tmp/tcga/filtered.zip',\n",
        "#                                     unzip=True)\n",
        "# !rm /tmp/tcga/filtered.zip\n",
        "\n",
        "# https://drive.google.com/file/d/1SmsFlYp2CHndQ4Jx_ngHtQfzIT6_4wfO/view?usp=sharing\n",
        "!pip install googledrivedownloader\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1SmsFlYp2CHndQ4Jx_ngHtQfzIT6_4wfO',\n",
        "                                    dest_path='/tmp/tcga/filtered.zip',\n",
        "                                    unzip=True)\n",
        "!rm /tmp/tcga/filtered.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQMu5KEfnfXO"
      },
      "source": [
        "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "# gdd.download_file_from_google_drive(file_id='1WCQ8l-Q-BAGnOczwMXWtmtHySvROaC14',\n",
        "#                                     dest_path='/tmp/tcga/has_been_moved_and_filtered',\n",
        "#                                     unzip=False)\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1ae3IfTucMSsUNEnC-HNbAaycDjIkI1T0',\n",
        "                                    dest_path='/tmp/tcga/has_been_moved_and_filtered',\n",
        "                                    unzip=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b47Qp4toni10"
      },
      "source": [
        "# filtered_tiles_output_folder = /tmp/tcga/filtered_tiles/\n",
        "has_been_filtered_filename = \"/tmp/tcga/has_been_moved_and_filtered\"\n",
        "data_folders = open(has_been_filtered_filename, 'r').read().splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6UZMkFwnmf-"
      },
      "source": [
        "slides_folders = list(map(lambda f: os.path.join(\"/tmp/tcga/\", f.replace(\"/Users/shunshao/Documents/GitHub/tcga_segmentation/output_folder/\", \"/tmp/tcga/\")), data_folders))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxzN02DLnnJo"
      },
      "source": [
        "len(slides_folders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPa0ozsmnr8p"
      },
      "source": [
        "slides_folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8sXJM0EnsWT"
      },
      "source": [
        "# Dataset\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, slides_folders, model_input_size, is_training, max_bag_size, logger, max_dataset_size=None,\n",
        "                 with_data_augmentation=True, seed=123, normalization_mean=None, normalization_std=None):\n",
        "        \"\"\"\n",
        "        :param slides_folders: list of abs paths of slide folder (which should contains images, summary/label/percent\n",
        "            files\n",
        "        :param model_input_size: expected model input size (for cropping)\n",
        "        :param is_training: True if is training, else False (for data augmentation)\n",
        "        :param max_bag_size: maximum number of instances to be returned per bag\n",
        "        \"\"\"\n",
        "\n",
        "        def verify_slide_folder_exists(slide_folder):\n",
        "            if not os.path.exists(slide_folder):\n",
        "                raise FileExistsError('parent dataset folder %s does not exist' % slide_folder)\n",
        "\n",
        "        list(map(verify_slide_folder_exists, slides_folders))\n",
        "\n",
        "        # self.slides_folders = np.asarray(slides_folders)\n",
        "        self.slides_folders = slides_folders\n",
        "        self.model_input_size = model_input_size\n",
        "        self.max_bag_size = max_bag_size\n",
        "        self.max_dataset_size = max_dataset_size\n",
        "\n",
        "        self.is_training = is_training\n",
        "\n",
        "        # self.logger = logger\n",
        "\n",
        "        self.slides_ids = []  # ids slides\n",
        "        self.slides_labels = []  # raw str labels\n",
        "        self.slides_summaries = []  # list of all initial tiles of slides\n",
        "        self.slides_cases = []  # list of all cases IDs\n",
        "        self.slides_images_filepaths = []  # list of all in-dataset tilespaths of slides\n",
        "\n",
        "        self.with_data_augmentation = with_data_augmentation\n",
        "        normalization_mean = (0, 0, 0) if normalization_mean is None else normalization_mean\n",
        "        normalization_std = (1, 1, 1) if normalization_std is None else normalization_std\n",
        "        self.transform = self._define_data_transforms(normalization_mean, normalization_std)\n",
        "\n",
        "        self.seed = seed\n",
        "\n",
        "        slides_ids, slides_labels, slides_summaries, slides_cases, slides_images_filepaths = self.load_data()\n",
        "        self.slides_ids = slides_ids\n",
        "        self.slides_labels = slides_labels\n",
        "        self.slides_summaries = slides_summaries\n",
        "        self.slides_cases = slides_cases\n",
        "        self.slides_images_filepaths = slides_images_filepaths\n",
        "\n",
        "        assert len(self.slides_ids) == len(self.slides_labels) == len(self.slides_summaries) == \\\n",
        "               len(self.slides_images_filepaths), 'mismatch in slides containers lengths %s' % (\n",
        "            ' '.join(str(len(l)) for l in [self.slides_ids, self.slides_labels, self.slides_summaries,\n",
        "                                           self.slides_images_filepaths]))\n",
        "\n",
        "        self.retrieve_tiles_ids_with_images = True  # True will return bag of images and associated tiles ids\n",
        "\n",
        "    def _define_data_transforms(self, mean, std):\n",
        "        if self.with_data_augmentation:\n",
        "            return transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.01),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ])\n",
        "        return transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "    def load_data(self):\n",
        "        slides_ids, slides_labels, slides_summaries, slides_cases, slides_images_filepaths = [], [], [], [], []\n",
        "\n",
        "        # Name of expected non-image files for all slides folders\n",
        "        label_filename = 'label.txt'\n",
        "        case_id_filename = 'case_id.txt'\n",
        "        summary_filename = 'summary.txt'\n",
        "\n",
        "        # Seek all slides folders, and load static data including list of tiles filepaths and bag label\n",
        "        for i, slide_folder in enumerate(tqdm(self.slides_folders)):\n",
        "            if self.max_dataset_size is not None and i + 1 > self.max_dataset_size:\n",
        "                break\n",
        "            # print(slide_folder)\n",
        "            # a = filter(lambda f: os.path.isfile(os.path.join(slide_folder, f)), os.listdir(slide_folder))\n",
        "            # print(a)\n",
        "            # b = list(a)\n",
        "            all_slide_files = list(filter(lambda f: os.path.isfile(os.path.join(slide_folder, f)),\n",
        "                                          os.listdir(slide_folder)))\n",
        "\n",
        "            # Seek and save label, case_id and summary files: expects 1 and only 1 for each\n",
        "            for data_filename in [label_filename, case_id_filename, summary_filename]:\n",
        "                assert sum([f == data_filename for f in all_slide_files]) == 1, \\\n",
        "                    'slide %s: found %d files for %s, expected 1' % (slide_folder,\n",
        "                                                                     sum([f == data_filename for f in\n",
        "                                                                          all_slide_files], ),\n",
        "                                                                     data_filename)\n",
        "\n",
        "            label_file = os.path.join(slide_folder, [f for f in all_slide_files if f == label_filename][0])\n",
        "            case_id_file = os.path.join(slide_folder, [f for f in all_slide_files if f == case_id_filename][0])\n",
        "            summary_file = os.path.join(slide_folder, [f for f in all_slide_files if f == summary_filename][0])\n",
        "            with open(label_file, 'r') as f:\n",
        "                slide_label = int(f.read())\n",
        "            with open(case_id_file, 'r') as f:\n",
        "                slide_case_id = f.read()\n",
        "            with open(summary_file, 'r') as f:\n",
        "                slide_original_tiles = f.read().splitlines()\n",
        "\n",
        "            # Seek all filtered images of slide (not-background images)\n",
        "            slide_images_filenames = list(filter(lambda f: f.endswith(('.jpeg', '.jpg', '.png')), all_slide_files))\n",
        "\n",
        "            if len(slide_images_filenames) == 0:\n",
        "                self.logger.warning('Discarding slide %s of class %d because there are no images' %\n",
        "                                    (slide_folder, slide_label))\n",
        "                continue\n",
        "\n",
        "            # Save data\n",
        "            slides_ids.append(os.path.basename(slide_folder))\n",
        "            slides_labels.append(slide_label)\n",
        "            slides_summaries.append(slide_original_tiles)\n",
        "            slides_cases.append(slide_case_id)\n",
        "            slides_images_filepaths.append(\n",
        "                list(map(lambda f: os.path.abspath(os.path.join(slide_folder, f)), slide_images_filenames)))\n",
        "\n",
        "        slides_ids = np.asarray(slides_ids)\n",
        "        slides_labels = np.asarray(slides_labels)\n",
        "        print(\"end\\n\")\n",
        "\n",
        "        return slides_ids, slides_labels, slides_summaries, slides_cases, slides_images_filepaths\n",
        "\n",
        "    # def show_bag(self, bag_idx, savefolder=None):\n",
        "    #     \"\"\" Plot/save tiles sampled from the slide of provided index \"\"\"\n",
        "    #     bag = self._get_slide_instances(bag_idx)\n",
        "    #     bag_label = self.slides_labels[bag_idx]\n",
        "    #     tr = transforms.ToTensor()\n",
        "    #     bag = [tr(b) for b in bag]\n",
        "    #     imgs = make_grid(bag)\n",
        "\n",
        "    #     npimgs = imgs.numpy()\n",
        "    #     plt.imshow(np.transpose(npimgs, (1, 2, 0)), interpolation='nearest')\n",
        "    #     plt.title('Bag label: %s | %d instances' % (bag_label, len(bag)))\n",
        "    #     if savefolder is not None:\n",
        "    #         plt.savefig(os.path.join(savefolder, 'show_' + str(bag_idx) + '.png'), dpi=1000)\n",
        "    #     else:\n",
        "    #         plt.show()\n",
        "\n",
        "    def _get_slide_instances(self, item):\n",
        "        \"\"\" Memory load all tiles or randomly sampled tiles from slide of specified index \"\"\"\n",
        "        slide_images_filepaths = self.slides_images_filepaths[item]\n",
        "\n",
        "        # Randomly sample the specified max number of tiles from the slide with replacement\n",
        "        if self.max_bag_size is not None:\n",
        "            slide_images_filepaths = random.choices(slide_images_filepaths, k=self.max_bag_size)\n",
        "        # print(f\"the self.slides_folders is {self.slides_folders}\")\n",
        "        # print(f\"the slides_labels is {self.slides_labels}\")\n",
        "        # print(f\"the self.slides_folders[item] is {self.slides_folders[item]}\")\n",
        "        # Load images\n",
        "        bag_images = [pil_loader(slide_image_filepath) for slide_image_filepath in slide_images_filepaths]\n",
        "        # print(f\"item is {item}\")\n",
        "        # print(f\"len self.slides_summaries[item] is {len(self.slides_summaries[item])}\")\n",
        "        if self.retrieve_tiles_ids_with_images:\n",
        "            # return bag of images as well as the associated ids of the tiles\n",
        "            return bag_images, list(map(os.path.basename, slide_images_filepaths)), self.slides_summaries[item], self.slides_folders[item]\n",
        "        return bag_images\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if not self.retrieve_tiles_ids_with_images:\n",
        "            slide_instances = self._get_slide_instances(item)\n",
        "            slide_instances = torch.stack([self.transform(instance) for instance in slide_instances])\n",
        "            slide_label = self.slides_labels[item]\n",
        "            return slide_instances, slide_label\n",
        "\n",
        "        slide_instances, tiles_ids, slide_summary, slides_folder = self._get_slide_instances(item)\n",
        "        slide_instances = torch.stack([self.transform(instance) for instance in slide_instances])\n",
        "        slide_label = self.slides_labels[item]\n",
        "        return slide_instances, slide_label, tiles_ids, slide_summary, slides_folder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slides_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq48CCVFnskR"
      },
      "source": [
        "import random\n",
        "\n",
        "def split_svs_samples_casewise(svs_files, associated_cases_ids, val_size, test_size, seed=123):\n",
        "    assert len(svs_files) == len(associated_cases_ids), 'Expected same number of SVS files than associated case ID'\n",
        "    random.seed(seed)\n",
        "    train_size = 1. - val_size - test_size\n",
        "\n",
        "    unique_cases_ids = list(set(associated_cases_ids))\n",
        "    random.shuffle(unique_cases_ids)\n",
        "    total_unique_cases_ids = len(unique_cases_ids)\n",
        "\n",
        "    # Extract cases ids for training, validation and testing sets\n",
        "    train_cases_ids = unique_cases_ids[:int(train_size*total_unique_cases_ids)]\n",
        "    val_cases_ids = unique_cases_ids[int(train_size*total_unique_cases_ids):\n",
        "                                     int(train_size*total_unique_cases_ids)+int(val_size*total_unique_cases_ids)]\n",
        "    test_cases_ids = unique_cases_ids[int(train_size*total_unique_cases_ids)+int(val_size*total_unique_cases_ids):]\n",
        "    assert len(train_cases_ids) + len(val_cases_ids) + len(test_cases_ids) == total_unique_cases_ids\n",
        "\n",
        "    # Compute associated split set for SVS files\n",
        "    train_svs_files, val_svs_files, test_svs_files = [], [], []\n",
        "    for svs_file, associated_case_id in zip(svs_files, associated_cases_ids):\n",
        "        if associated_case_id in train_cases_ids:\n",
        "            train_svs_files.append(svs_file)\n",
        "        elif associated_case_id in val_cases_ids:\n",
        "            val_svs_files.append(svs_file)\n",
        "        else:\n",
        "            test_svs_files.append(svs_file)\n",
        "\n",
        "    return train_svs_files, val_svs_files, test_svs_files\n",
        "\n",
        "def build_datasets(source_slides_folders, model_input_width, hyper_parameters, logger):\n",
        "    normalization_channels_mean = (0.6387467, 0.51136744, 0.6061169)\n",
        "    normalization_channels_std = (0.31200314, 0.3260718, 0.30386254)\n",
        "\n",
        "    # First load all data into a single Dataset\n",
        "    whole_dataset = Dataset(slides_folders=source_slides_folders, model_input_size=model_input_width,\n",
        "                            is_training=False, max_bag_size=hyper_parameters['max_bag_size'],\n",
        "                            logger=logger, max_dataset_size=hyper_parameters['dataset_max_size'],\n",
        "                            with_data_augmentation=hyper_parameters['with_data_augmentation'],\n",
        "                            seed=hyper_parameters['seed'],\n",
        "                            normalization_mean=normalization_channels_mean,\n",
        "                            normalization_std=normalization_channels_std)\n",
        "    whole_cases_ids = whole_dataset.slides_cases\n",
        "    whole_indexes = list(range(len(whole_dataset)))\n",
        "\n",
        "    val_size = hyper_parameters['val_size']\n",
        "    test_size = hyper_parameters['test_size']\n",
        "    train_idx, val_idx, test_idx = split_svs_samples_casewise(whole_indexes, whole_cases_ids,\n",
        "                                                              val_size=val_size, test_size=test_size,\n",
        "                                                              seed=hyper_parameters['seed'])\n",
        "\n",
        "    val_dataset = torch.utils.data.Subset(whole_dataset, val_idx)\n",
        "    test_dataset = torch.utils.data.Subset(whole_dataset, test_idx)\n",
        "    train_dataset = torch.utils.data.Subset(whole_dataset, train_idx)\n",
        "    train_dataset.dataset.is_training = True\n",
        "    train_dataset.dataset.transform = train_dataset.dataset._define_data_transforms(normalization_channels_mean,\n",
        "                                                                                    normalization_channels_std)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, whole_cases_ids, whole_indexes, whole_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQGrys3wKcoS"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0yJLfEtM2jg"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAjptaLdN2o6"
      },
      "source": [
        "# for batch_idx, (data, bag_label) in enumerate(train_dataloader):\n",
        "#   print(f\"batch_idx is {batch_idx}, batch size :{data.size(1)}, input size: {data.size(-1)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl-NudceKicY"
      },
      "source": [
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, pretrained=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pretrained = pretrained\n",
        "        self.model = models.resnet50(pretrained=self.pretrained)\n",
        "        self.model.fc = nn.Sequential(nn.Linear(2048, 2),) \n",
        "        self.output_activation = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logit = self.model(x)\n",
        "        output = self.output_activation(logit)\n",
        "\n",
        "        return output, logit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlx3ybz6za-U"
      },
      "source": [
        "## cMIL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfY2T5qbQevi"
      },
      "source": [
        "class cMIL(object):\n",
        "    \"\"\"\n",
        "    criterion: \n",
        "              max-max: select the data with highest response as the representative of the image, regardless of the image class.\n",
        "              max-min: select the data with highest response as the representative of the image for class 1, and those with lowest\n",
        "                       response to represent class 0.\n",
        "\n",
        "    \"\"\"\n",
        "    purpose_set = {'train', 'valid', 'infer'}\n",
        "    criterion_set = {'max-max', 'max-min'}\n",
        "\n",
        "    def __init__(self, writer, criterion, train_loader, val_loader, full_loader, save_path=None, pretrained=False, save_interval=1000):\n",
        "        # self.data_dict_path = data_dict_path\n",
        "        self.criterion = criterion\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.pretrained = pretrained\n",
        "        # self.bag_size = bag_size\n",
        "        # self.batch_size = batch_size\n",
        "        # self.valid_ratio = valid_ratio\n",
        "\n",
        "        # self.gpu_num = gpu_num\n",
        "        # self.worker_ratio = worker_ratio\n",
        "\n",
        "        self.save_interval = save_interval\n",
        "\n",
        "        # Intialize dataloader\n",
        "        # self.kwargs = {'num_workers': self.gpu_num * self.worker_ratio, 'pin_memory': False} if torch.cuda.is_available() else {}\n",
        "        # self.kwargs = {} if torch.cuda.is_available() else {}\n",
        "\n",
        "        # self.dataset_train = ImageBagDataset(self.data_dict_path, bag_size=self.bag_size, purpose='train', valid_ratio=self.valid_ratio)\n",
        "        # self.dataset_val = ImageBagDataset(self.data_dict_path, bag_size=self.bag_size, purpose='valid', valid_ratio=self.valid_ratio)\n",
        "        # self.dataset_full = ImageBagDataset(self.data_dict_path, bag_size=self.bag_size, purpose='full')\n",
        "\n",
        "        # self.training_set_size = train_loader.dataset.__len__()\n",
        "        # self.valid_set_size = val_loader.dataset.__len__()\n",
        "        # self.full_set_size = full_loader.dataset.__len__()\n",
        "        \n",
        "        self.training_set_size = 0\n",
        "        self.valid_set_size = 0\n",
        "        self.full_set_size = 0\n",
        "\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.full_loader = full_loader\n",
        "\n",
        "        self.best_acc = 0\n",
        "        self.best_model_wts = 0\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = ResNet50(pretrained=self.pretrained)\n",
        "        self.loss_func = nn.NLLLoss()\n",
        "\n",
        "        self.writer = writer\n",
        "\n",
        "    def load_model(self, best_model_wts):\n",
        "        self.model = ResNet50(pretrained=False)\n",
        "        self.model.load_state_dict(best_model_wts)\n",
        "\n",
        "    def train(self, epoch_num, lr=1e-4):\n",
        "        self.training_set_size = self.train_loader.dataset.__len__()\n",
        "\n",
        "        # Instantiate optimizer\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "            # if self.multi_gpu:\n",
        "            #     self.model = nn.DataParallel(self.model)\n",
        "        best_acc = 0\n",
        "        for epoch in range(1, epoch_num + 1):\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "            self.forward_and_backward(purpose='train', epoch=epoch)\n",
        "\n",
        "            print(\"Epoch took {} secs.\".format(time.time() - epoch_start_time))\n",
        "        \n",
        "        print(f'best_acc is {self.best_acc}')\n",
        "        \n",
        "        return self.best_model_wts\n",
        "        # ?\n",
        "        # if isinstance(self.model, nn.DataParallel):\n",
        "        #     model = self.model.module\n",
        "        # else:\n",
        "        #     model = self.model\n",
        "\n",
        "        # ?\n",
        "        # model_cpu = model.cpu()\n",
        "\n",
        "    def validate(self):\n",
        "        self.valid_set_size = self.val_loader.dataset.__len__()\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss, auc, acc, sens, prec = self.forward_and_backward(purpose='valid')\n",
        "\n",
        "        return loss, auc, acc, sens, prec\n",
        "\n",
        "    def infer(self, data_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "            # if self.multi_gpu:\n",
        "            #     self.model = nn.DataParallel(self.model)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            infer_total_list = self.forward_and_backward(purpose='infer', data_loader=data_loader)\n",
        "\n",
        "        # return infer_total_list\n",
        "        return infer_total_list\n",
        "        \n",
        "    def forward_and_backward(self, purpose='train', epoch=1, data_loader=None):\n",
        "        init_time = time.time()\n",
        "\n",
        "        if purpose not in self.purpose_set:\n",
        "            raise ValueError(\"Invlid purpose given!\")\n",
        "\n",
        "        auc_list = []\n",
        "        correct_list = []\n",
        "\n",
        "        nc_total = 0\n",
        "        c_total = 0\n",
        "        correct = 0\n",
        "        num_rows = 0\n",
        "        test_loss = 0\n",
        "        rep_list = []\n",
        "\n",
        "        if purpose == 'train':\n",
        "            data_loader = self.train_loader\n",
        "            self.model.train()\n",
        "\n",
        "            print()\n",
        "            print(\"Training...\")\n",
        "            print(\"Training set size: {}\".format(len(data_loader.dataset)))\n",
        "            loss_tensorboard = 0\n",
        "            acc_tensorboard = 0\n",
        "        elif purpose == 'valid':\n",
        "            data_loader = self.val_loader\n",
        "            self.model.eval()\n",
        "\n",
        "            print()\n",
        "            print(\"Validating...\")\n",
        "            print(\"Test set size: {}\".format(len(data_loader.dataset)))\n",
        "\n",
        "            pred_total_list = []\n",
        "            target_total_list = []\n",
        "        elif purpose == 'infer':\n",
        "            infer_total_list = []\n",
        "\n",
        "            self.model.eval()\n",
        "\n",
        "            print()\n",
        "            print(\"Inferring...\")\n",
        "            print(\"Test set size: {}\".format(len(data_loader.dataset)))\n",
        "\n",
        "            pbar = tqdm(total=len(data_loader.dataset))\n",
        "\n",
        "        # for batch_idx, (data, target, index) in enumerate(data_loader):\n",
        "        # for batch_idx, (data, target, _, _) in enumerate(data_loader):\n",
        "        for batch_idx, (data, target, tiles_ids, slide_summary, slides_folder) in enumerate(data_loader):\n",
        "            # print(f\"the slides_folder is {slides_folder}\")\n",
        "            # print(f\"the number of tiles is {len(slide_summary)}\")\n",
        "            index = tiles_ids\n",
        "            train_start_time = time.time()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                # print(data)\n",
        "                # print(target)\n",
        "                data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "                # data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            if purpose == 'train':\n",
        "                data, target = data.requires_grad_(), target\n",
        "                self.optimizer.zero_grad()\n",
        "            \n",
        "            batch_size, bag_size, input_size = data.size(0), data.size(1), data.size(-1)\n",
        "            num_rows += batch_size\n",
        "            # print(f\"the batch size is {batch_size}, the input size is {input_size}\")\n",
        "            # data = data.view(batch_size * self.bag_size, 3, input_size, input_size)\n",
        "            # print(f\"the data before squeeze size is {data.size()}\")\n",
        "            data = torch.squeeze(data, 0)\n",
        "            # print(f\"the data after squeeze size is {data.size()}\")\n",
        "            output, logit = self.model(data)\n",
        "            # print(f\"output size is {output.size()}\")\n",
        "            output = output.view(batch_size, bag_size, 2)\n",
        "\n",
        "            # index_list = index.cpu().tolist()\n",
        "            selected_idx = 0\n",
        "            selected_idx_list = []\n",
        "\n",
        "            if self.criterion == 'max-max':\n",
        "                # selected_idx_list = [bag[:, 1].max(0)[1] for _, bag in enumerate(output)]\n",
        "                # print(f\"the output have size {output.size()}\")\n",
        "                for _, bag in enumerate(output):\n",
        "                    # print(f\"the bag has size {bag.size()}\")\n",
        "                    # print(f\"the bag is {bag}\")\n",
        "                    maxmax_indices = torch.nonzero(bag[:, 1] == bag[:, 1].max(0)[0])\n",
        "                    # print(f\"the bag bag[:, 1].max(0) {bag[:, 1].max(0)}\")\n",
        "                    # print(f\"the bag bag[:, 1].max(0)[0] {bag[:, 1].max(0)[0]}\")\n",
        "                    # print(f\"the bag bag filtered {bag[:, 1] == bag[:, 1].max(0)[0]}\")\n",
        "                    # print(f\"the maxmax_indices is {maxmax_indices}, the length the of maxmax_indices is {len(maxmax_indices)}\")\n",
        "                    pick_idx = np.random.randint(len(maxmax_indices))\n",
        "                    # print(f\"the pick_idx is {pick_idx}\")\n",
        "                    selected_idx = maxmax_indices[pick_idx].squeeze(0)\n",
        "                    # print(f\"the selected_idx is {selected_idx}\")\n",
        "                    selected_idx_list.append(selected_idx)\n",
        "            else:\n",
        "                # selected_idx_list = [bag[:, 1].max(0)[1] if target[i].item() == 1 else bag[:, 1].min(0)[1] \\\n",
        "                #                      for i, bag in enumerate(output)]\n",
        "                # print(f\"the output have size {output.size()}\")\n",
        "                for i, bag in enumerate(output):\n",
        "                    # print(f\"the bag has size {bag.size()}\")\n",
        "                    maxmin_indices = torch.nonzero(bag[:, 1] == bag[:, 1].max(0)[0]) if target[i].item() == 1 \\\n",
        "                                else torch.nonzero(bag[:, 1] == bag[:, 1].min(0)[0])\n",
        "\n",
        "                    pick_idx = np.random.randint(len(maxmin_indices))\n",
        "                    selected_idx = maxmin_indices[pick_idx].squeeze(0)\n",
        "                    selected_idx_list.append(selected_idx)\n",
        "\n",
        "\n",
        "            selected_torch = torch.stack([output[i][idx] for i, idx in enumerate(selected_idx_list)])\n",
        "\n",
        "            prediction = selected_torch.max(1)[1]\n",
        "            correct = (prediction == target).sum().cpu().item()\n",
        "\n",
        "            pred_cpu = prediction.cpu().detach().numpy()\n",
        "            # print(f\"pred_cpu is {pred_cpu}\")\n",
        "            target_cpu = target.cpu().detach().numpy()\n",
        "            # print(f\"target_cpu is {target_cpu}\")\n",
        "            selected_exp_cpu = torch.exp(selected_torch[:, 1]).cpu().detach().numpy()\n",
        "            # print(f\"selected_idx_list is {selected_idx_list}\")\n",
        "            # index_list = [index[i][0] for i in range(len(index))]\n",
        "            # print(index_list)\n",
        "            # return index_list, selected_exp_cpu\n",
        "            if purpose == 'infer':\n",
        "                pick_list = list(zip(slides_folder, tiles_ids[selected_idx], pred_cpu.tolist(), target_cpu.tolist()))\n",
        "                # print(f\"The selected_idx is {selected_idx}\")\n",
        "                # print(f\"the image is {tiles_ids[selected_idx]}\")\n",
        "                # print(f\"the slides_foler is {slides_folder}\")\n",
        "                # print(f\"the pick_list is {pick_list}\")\n",
        "                print()\n",
        "                file_path = slides_folder[0] \n",
        "                file_path += '/'\n",
        "                file_path += tiles_ids[selected_idx][0]\n",
        "                print(file_path)\n",
        "                print(f\"The most probable cancer slides predicted by {purpose} in the case {slides_folder} is {tiles_ids[selected_idx]}, display as below\")\n",
        "                display(pil_loader(file_path))\n",
        "                infer_total_list += pick_list\n",
        "                pbar.update(batch_size)\n",
        "            else:\n",
        "                loss_sum = self.loss_func(selected_torch, target)\n",
        "\n",
        "            if purpose == 'valid':\n",
        "                try:\n",
        "                    auc = roc_auc_score(target_cpu, selected_exp_cpu)\n",
        "                except:\n",
        "                    pass\n",
        "                else:\n",
        "                    auc_list.append(auc)\n",
        "\n",
        "                test_loss += loss_sum\n",
        "                correct_list.append(correct)\n",
        "\n",
        "                target_list = target_cpu.tolist()\n",
        "                pred_list = pred_cpu.tolist()\n",
        "\n",
        "                non_cancer_count, cancer_count = target_list.count(0), target_list.count(1)\n",
        "\n",
        "                nc_total += non_cancer_count\n",
        "                c_total += cancer_count\n",
        "\n",
        "                target_total_list += target_list\n",
        "                pred_total_list += pred_list\n",
        "\n",
        "            if purpose == 'train':\n",
        "                loss_sum.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                print('Epoch({}-{}) [{}/{} ({:.0f}%)] Loss: {:.6f} Acc: {:.3f} Output: {} Prediction: {} Label: {} ({} sec/step)'.format(\n",
        "                    epoch, batch_idx,\n",
        "                    batch_idx * batch_size, self.training_set_size, 100. * batch_idx * batch_size / self.training_set_size, \n",
        "                    loss_sum.data.item(),\n",
        "                    correct / batch_size,\n",
        "                    list(map(lambda x: round(x, 2), selected_exp_cpu.tolist()[:4])),\n",
        "                    list(map(lambda x: round(x, 2), pred_cpu.tolist()[:4])),\n",
        "                    target_cpu.tolist()[:4],\n",
        "                    round(time.time() - train_start_time, 3)\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                loss_tensorboard += loss_sum.data.item()\n",
        "                acc_tensorboard += correct\n",
        "\n",
        "            elif purpose == 'valid':\n",
        "                print('Val/Infer [{}/{} ({:.0f}%)] Loss: {:.6f} Acc: {:.6f} Output: {} Prediction: {} Label: {} ({} sec/step)'.format(\n",
        "                    batch_idx * batch_size, self.valid_set_size, 100. * batch_idx * batch_size / self.valid_set_size, \n",
        "                    loss_sum.data.item(),\n",
        "                    correct / batch_size,\n",
        "                    list(map(lambda x: round(x, 2), selected_exp_cpu.tolist()[:4])),\n",
        "                    list(map(lambda x: round(x, 2), pred_cpu.tolist()[:4])),\n",
        "                    target_cpu.tolist()[:4],\n",
        "                    round(time.time() - train_start_time, 3)\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        if purpose == 'valid':\n",
        "            test_loss /= num_rows\n",
        "            auc = np.mean(auc_list)\n",
        "            num_correct = np.sum(correct_list)\n",
        "            acc = (100. * num_correct) / num_rows\n",
        "            # print(f\"target is {np.array(target_total_list)}\")\n",
        "            # print(f\"pred is {np.array(pred_total_list)}\")\n",
        "            sens = precision_score(np.array(target_total_list), np.array(pred_total_list))\n",
        "            cm = confusion_matrix(np.array(target_total_list), np.array(pred_total_list))\n",
        "            tn = cm[0, 0]\n",
        "            tp = cm[1, 1]\n",
        "            fn = cm[1, 0]\n",
        "            fp = cm[0, 1]\n",
        "            spec = tn / (tn + fp)\n",
        "            prec = precision_score(np.array(target_total_list), np.array(pred_total_list))\n",
        "\n",
        "            # fpr = fp/(fp + tn)\n",
        "            # tpr = sens\n",
        "\n",
        "            # roc_auc = auc(fpr, tpr)\n",
        "            # plt.figure()\n",
        "            # lw = 2\n",
        "            # plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % auc)\n",
        "            # plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "            # plt.xlim([0.0, 1.0])\n",
        "            # plt.ylim([0.0, 1.05])\n",
        "            # plt.xlabel('False Positive Rate')\n",
        "            # plt.ylabel('True Positive Rate')\n",
        "            # plt.title('Receiver operating characteristic example')\n",
        "            # plt.legend(loc=\"lower right\")\n",
        "            # plt.show()\n",
        "            print(f'tn is {tn}, tp is {tp}, fn is {fn}, fp is {fp}')\n",
        "            print(\"Validation: Average loss: {:.4f}, AUC: {:.3f}, Acc: {}/{} ({:.3f}%) Recall: {:.3f} Prec: {:.3f} Spec: {}\\n\".format(\n",
        "                  test_loss, \n",
        "                  auc, \n",
        "                  num_correct, num_rows, \n",
        "                  acc,\n",
        "                  sens,\n",
        "                  prec,\n",
        "                  spec\n",
        "                  ))\n",
        "            # try:\n",
        "            fpr, tpr, _ = roc_curve(np.array(target_total_list), np.array(pred_total_list))\n",
        "            # print(f\"Roc calculation result: fpr is {fpr}, tpr is {tpr}, {type(fpr)}, {type(tpr)}\")\n",
        "            # roc_fpr[:, epoch] = fpr\n",
        "            # roc_tpr[:, epoch] = tpr\n",
        "            roc_auc = auc_area_calc(fpr, tpr)\n",
        "            plt.figure()\n",
        "            lw = 2\n",
        "            plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title('Receiver operating characteristic example')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.show()\n",
        "            # except TypeError:\n",
        "            #     pass\n",
        "            acc /= 100\n",
        "            return test_loss.cpu().detach().numpy(), auc, acc, sens, prec\n",
        "\n",
        "        if purpose == 'infer':\n",
        "            return infer_total_list\n",
        "        \n",
        "        if purpose == 'train':\n",
        "            val_loss, val_auc, val_acc, val_sens, val_prec = self.validate()\n",
        "            with self.writer.as_default():\n",
        "                loss_tensorboard /= num_rows\n",
        "                error = 1 - (acc_tensorboard / num_rows)\n",
        "                print(f\"summary val acc is {val_acc}\")\n",
        "                tf.summary.scalar('Loss/Train', loss_tensorboard, step=epoch)\n",
        "                tf.summary.scalar('Error/Train', error, step=epoch)\n",
        "                tf.summary.scalar('Loss/Val', val_loss, step=epoch)\n",
        "                tf.summary.scalar('Error/Val', (1 - val_acc), step=epoch)\n",
        "\n",
        "            if val_acc > self.best_acc:\n",
        "                print(f\"acc update {val_acc}\")\n",
        "                self.best_acc = val_acc\n",
        "                self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "            # print(\"{} secs elapsed.\".format(time.time() - init_time))\n",
        "\n",
        "            # val_auc, val_acc, val_sens, val_prec = self.validate()\n",
        "\n",
        "            # if self.save_path is not None:\n",
        "            #     if isinstance(self.model, nn.DataParallel):\n",
        "            #         model = self.model.module\n",
        "            #     else:\n",
        "            #         model = self.model\n",
        "                \n",
        "            #     model_cpu = model.cpu()\n",
        "\n",
        "            #     torch.save(model_cpu.state_dict(), self.save_path + f'{epoch}_{batch_idx}_{str(val_auc)[:5]}_{str(val_acc)[:5]}_{str(val_sens)[:5]}_{str(val_prec)[:5]}.pth')\n",
        "\n",
        "            #     if torch.cuda.is_available():\n",
        "            #         self.model = self.model.cuda()\n",
        "\n",
        "# (self, data_dict_path, criterion, save_path=None, pretrained=False, bag_size=16, batch_size=1, valid_ratio=0.01, gpu_num=1, worker_ratio=4, save_interval=1000)\n",
        "\n",
        "#     max_max = cMIL(DATA_DICT_PATH, criterion=CRITERION, save_path=SAVE_PATH, pretrained=True, bag_size=BAG_SIZE, \n",
        "#                    batch_size=BATCH_SIZE, gpu_num=GPU_NUM, worker_ratio=WORKER_RATIO, valid_ratio=VALID_RATIO,\n",
        "#                    save_interval=SAVE_INTERVAL)\n",
        "\n",
        "# (self, criterion, train_loader, val_loader, full_loader, save_path=None, pretrained=False, bag_size=16, batch_size=1, worker_ratio=4, save_interval=1000)\n",
        "\n",
        "#     max_max = cMIL(criterion=CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, bag_size=BAG_SIZE, \n",
        "#                 batch_size=BATCH_SIZE, save_interval=SAVE_INTERVAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arJYfGmPd0-9"
      },
      "source": [
        "# class cMIL(object):\n",
        "#     \"\"\"\n",
        "#     criterion: \n",
        "#               max-max: select the data with highest response as the representative of the image, regardless of the image class.\n",
        "#               max-min: select the data with highest response as the representative of the image for class 1, and those with lowest\n",
        "#                        response to represent class 0.\n",
        "\n",
        "#     \"\"\"\n",
        "#     purpose_set = {'train', 'valid', 'infer'}\n",
        "#     criterion_set = {'max-max', 'max-min'}\n",
        "\n",
        "#     def __init__(self, writer, criterion, train_loader, val_loader, full_loader, save_path=None, pretrained=False, save_interval=1000):\n",
        "#         # self.data_dict_path = data_dict_path\n",
        "#         self.criterion = criterion\n",
        "#         self.save_path = save_path\n",
        "\n",
        "#         self.pretrained = pretrained\n",
        "#         # self.bag_size = bag_size\n",
        "#         # self.batch_size = batch_size\n",
        "#         # self.valid_ratio = valid_ratio\n",
        "\n",
        "#         # self.gpu_num = gpu_num\n",
        "#         # self.worker_ratio = worker_ratio\n",
        "\n",
        "#         self.save_interval = save_interval\n",
        "\n",
        "#         # Intialize dataloader\n",
        "#         # self.kwargs = {'num_workers': self.gpu_num * self.worker_ratio, 'pin_memory': False} if torch.cuda.is_available() else {}\n",
        "#         # self.kwargs = {} if torch.cuda.is_available() else {}\n",
        "\n",
        "#         # self.dataset_train = ImageBagDataset(self.data_dict_path, bag_size=self.bag_size, purpose='train', valid_ratio=self.valid_ratio)\n",
        "#         # self.dataset_val = ImageBagDataset(self.data_dict_path, bag_size=self.bag_size, purpose='valid', valid_ratio=self.valid_ratio)\n",
        "#         # self.dataset_full = ImageBagDataset(self.data_dict_path, bag_size=self.bag_size, purpose='full')\n",
        "\n",
        "#         # self.training_set_size = train_loader.dataset.__len__()\n",
        "#         # self.valid_set_size = val_loader.dataset.__len__()\n",
        "#         # self.full_set_size = full_loader.dataset.__len__()\n",
        "        \n",
        "#         self.training_set_size = 0\n",
        "#         self.valid_set_size = 0\n",
        "#         self.full_set_size = 0\n",
        "\n",
        "#         self.train_loader = train_loader\n",
        "#         self.val_loader = val_loader\n",
        "#         self.full_loader = full_loader\n",
        "\n",
        "#         self.best_acc = 0\n",
        "#         self.best_model_wts = 0\n",
        "\n",
        "#         # Initialize model\n",
        "#         self.model = ResNet50(pretrained=self.pretrained)\n",
        "#         self.loss_func = nn.NLLLoss()\n",
        "\n",
        "#         self.writer = writer\n",
        "\n",
        "#     def load_model(self, best_model_wts):\n",
        "#         self.model = ResNet50(pretrained=False)\n",
        "#         self.model.load_state_dict(best_model_wts)\n",
        "\n",
        "#     def train(self, epoch_num, lr=1e-4):\n",
        "#         self.training_set_size = self.train_loader.dataset.__len__()\n",
        "\n",
        "#         # Instantiate optimizer\n",
        "#         self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "#         if torch.cuda.is_available():\n",
        "#             self.model = self.model.cuda()\n",
        "\n",
        "#             # if self.multi_gpu:\n",
        "#             #     self.model = nn.DataParallel(self.model)\n",
        "#         best_acc = 0\n",
        "#         for epoch in range(1, epoch_num + 1):\n",
        "#             epoch_start_time = time.time()\n",
        "\n",
        "#             self.forward_and_backward(purpose='train', epoch=epoch)\n",
        "\n",
        "#             print(\"Epoch took {} secs.\".format(time.time() - epoch_start_time))\n",
        "        \n",
        "#         print(f'best_acc is {self.best_acc}')\n",
        "        \n",
        "#         return self.best_model_wts\n",
        "#         # ?\n",
        "#         # if isinstance(self.model, nn.DataParallel):\n",
        "#         #     model = self.model.module\n",
        "#         # else:\n",
        "#         #     model = self.model\n",
        "\n",
        "#         # ?\n",
        "#         # model_cpu = model.cpu()\n",
        "\n",
        "#     def validate(self):\n",
        "#         self.valid_set_size = self.val_loader.dataset.__len__()\n",
        "#         if torch.cuda.is_available():\n",
        "#             self.model = self.model.cuda()\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             loss, auc, acc, sens, prec = self.forward_and_backward(purpose='valid')\n",
        "\n",
        "#         return loss, auc, acc, sens, prec\n",
        "\n",
        "#     def infer(self, data_loader):\n",
        "#         if torch.cuda.is_available():\n",
        "#             self.model = self.model.cuda()\n",
        "\n",
        "#         if torch.cuda.is_available():\n",
        "#             self.model = self.model.cuda()\n",
        "\n",
        "#             if self.multi_gpu:\n",
        "#                 self.model = nn.DataParallel(self.model)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             infer_total_list = self.forward_and_backward(purpose='infer', data_loader=data_loader)\n",
        "\n",
        "#         return infer_total_list\n",
        "        \n",
        "#     def forward_and_backward(self, purpose='train', epoch=1, data_loader=None):\n",
        "#         init_time = time.time()\n",
        "\n",
        "#         if purpose not in self.purpose_set:\n",
        "#             raise ValueError(\"Invlid purpose given!\")\n",
        "\n",
        "#         auc_list = []\n",
        "#         correct_list = []\n",
        "\n",
        "#         nc_total = 0\n",
        "#         c_total = 0\n",
        "#         correct = 0\n",
        "#         num_rows = 0\n",
        "#         test_loss = 0\n",
        "#         rep_list = []\n",
        "\n",
        "#         if purpose == 'train':\n",
        "#             data_loader = self.train_loader\n",
        "#             self.model.train()\n",
        "\n",
        "#             print()\n",
        "#             print(\"Training...\")\n",
        "#             print(\"Training set size: {}\".format(len(data_loader.dataset)))\n",
        "#             loss_tensorboard = 0\n",
        "#             acc_tensorboard = 0\n",
        "#         elif purpose == 'valid':\n",
        "#             data_loader = self.val_loader\n",
        "#             self.model.eval()\n",
        "\n",
        "#             print()\n",
        "#             print(\"Validating...\")\n",
        "#             print(\"Test set size: {}\".format(len(data_loader.dataset)))\n",
        "\n",
        "#             pred_total_list = []\n",
        "#             target_total_list = []\n",
        "#         elif purpose == 'infer':\n",
        "#             infer_total_list = []\n",
        "\n",
        "#             self.model.eval()\n",
        "\n",
        "#             print()\n",
        "#             print(\"Inferring...\")\n",
        "#             print(\"Test set size: {}\".format(len(data_loader.dataset)))\n",
        "\n",
        "#             pbar = tqdm(total=len(data_loader.dataset))\n",
        "\n",
        "#         # for batch_idx, (data, target, index) in enumerate(data_loader):\n",
        "#         for batch_idx, (data, target, _, _) in enumerate(data_loader):\n",
        "#             train_start_time = time.time()\n",
        "\n",
        "#             if torch.cuda.is_available():\n",
        "#                 # print(data)\n",
        "#                 # print(target)\n",
        "#                 data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "#                 # data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "#             if purpose == 'train':\n",
        "#                 data, target = data.requires_grad_(), target\n",
        "#                 self.optimizer.zero_grad()\n",
        "            \n",
        "#             batch_size, bag_size, input_size = data.size(0), data.size(1), data.size(-1)\n",
        "#             num_rows += batch_size\n",
        "#             # print(f\"the batch size is {batch_size}, the input size is {input_size}\")\n",
        "#             # data = data.view(batch_size * self.bag_size, 3, input_size, input_size)\n",
        "#             data = torch.squeeze(data, 0)\n",
        "#             output, logit = self.model(data)\n",
        "#             # print(f\"output size is {output.size()}\")\n",
        "#             output = output.view(batch_size, bag_size, 2)\n",
        "\n",
        "#             # index_list = index.cpu().tolist()\n",
        "#             selected_idx_list = []\n",
        "\n",
        "#             if self.criterion == 'max-max':\n",
        "#                 # selected_idx_list = [bag[:, 1].max(0)[1] for _, bag in enumerate(output)]\n",
        "#                 # print(f\"the output have size {output.size()}\")\n",
        "#                 for _, bag in enumerate(output):\n",
        "#                     print(f\"the bag has size {bag.size()}\")\n",
        "#                     print(f\"the bag is {bag}\")\n",
        "#                     maxmax_indices = torch.nonzero(bag[:, 1] == bag[:, 1].max(0)[0])\n",
        "\n",
        "#                     pick_idx = np.random.randint(len(maxmax_indices))\n",
        "#                     selected_idx = maxmax_indices[pick_idx].squeeze(0)\n",
        "\n",
        "#                     selected_idx_list.append(selected_idx)\n",
        "#             else:\n",
        "#                 # selected_idx_list = [bag[:, 1].max(0)[1] if target[i].item() == 1 else bag[:, 1].min(0)[1] \\\n",
        "#                 #                      for i, bag in enumerate(output)]\n",
        "#                 # print(f\"the output have size {output.size()}\")\n",
        "#                 for i, bag in enumerate(output):\n",
        "#                     # print(f\"the bag has size {bag.size()}\")\n",
        "#                     maxmin_indices = torch.nonzero(bag[:, 1] == bag[:, 1].max(0)[0]) if target[i].item() == 1 \\\n",
        "#                                 else torch.nonzero(bag[:, 1] == bag[:, 1].min(0)[0])\n",
        "\n",
        "#                     pick_idx = np.random.randint(len(maxmin_indices))\n",
        "#                     selected_idx = maxmin_indices[pick_idx].squeeze(0)\n",
        "\n",
        "#                     selected_idx_list.append(selected_idx)\n",
        "\n",
        "#             selected_torch = torch.stack([output[i][idx] for i, idx in enumerate(selected_idx_list)])\n",
        "\n",
        "#             prediction = selected_torch.max(1)[1]\n",
        "#             correct = (prediction == target).sum().cpu().item()\n",
        "\n",
        "#             pred_cpu = prediction.cpu().detach().numpy()\n",
        "#             # print(f\"pred_cpu is {pred_cpu}\")\n",
        "#             target_cpu = target.cpu().detach().numpy()\n",
        "#             # print(f\"target_cpu is {target_cpu}\")\n",
        "#             selected_exp_cpu = torch.exp(selected_torch[:, 1]).cpu().detach().numpy()\n",
        "#             print(f\"selected_idx_list is {selected_idx_list}\")\n",
        "\n",
        "#             if purpose == 'infer':\n",
        "#                 pick_list = list(zip(index_list, [idx.cpu().item() for idx in selected_idx_list], pred_cpu.tolist(), target_cpu.tolist()))\n",
        "#                 infer_total_list += pick_list\n",
        "#                 pbar.update(batch_size)\n",
        "#             else:\n",
        "#                 loss_sum = self.loss_func(selected_torch, target)\n",
        "\n",
        "#             if purpose == 'valid':\n",
        "#                 try:\n",
        "#                     auc = roc_auc_score(target_cpu, selected_exp_cpu)\n",
        "#                 except:\n",
        "#                     pass\n",
        "#                 else:\n",
        "#                     auc_list.append(auc)\n",
        "\n",
        "#                 test_loss += loss_sum\n",
        "#                 correct_list.append(correct)\n",
        "\n",
        "#                 target_list = target_cpu.tolist()\n",
        "#                 pred_list = pred_cpu.tolist()\n",
        "\n",
        "#                 non_cancer_count, cancer_count = target_list.count(0), target_list.count(1)\n",
        "\n",
        "#                 nc_total += non_cancer_count\n",
        "#                 c_total += cancer_count\n",
        "\n",
        "#                 target_total_list += target_list\n",
        "#                 pred_total_list += pred_list\n",
        "\n",
        "#             if purpose == 'train':\n",
        "#                 loss_sum.backward()\n",
        "#                 self.optimizer.step()\n",
        "\n",
        "#                 print('Epoch({}-{}) [{}/{} ({:.0f}%)] Loss: {:.6f} Acc: {:.3f} Output: {} Prediction: {} Label: {} ({} sec/step)'.format(\n",
        "#                     epoch, batch_idx,\n",
        "#                     batch_idx * batch_size, self.training_set_size, 100. * batch_idx * batch_size / self.training_set_size, \n",
        "#                     loss_sum.data.item(),\n",
        "#                     correct / batch_size,\n",
        "#                     list(map(lambda x: round(x, 2), selected_exp_cpu.tolist()[:4])),\n",
        "#                     list(map(lambda x: round(x, 2), pred_cpu.tolist()[:4])),\n",
        "#                     target_cpu.tolist()[:4],\n",
        "#                     round(time.time() - train_start_time, 3)\n",
        "#                     )\n",
        "#                 )\n",
        "\n",
        "#                 loss_tensorboard += loss_sum.data.item()\n",
        "#                 acc_tensorboard += correct\n",
        "\n",
        "#             elif purpose == 'valid':\n",
        "#                 print('Val/Infer [{}/{} ({:.0f}%)] Loss: {:.6f} Acc: {:.6f} Output: {} Prediction: {} Label: {} ({} sec/step)'.format(\n",
        "#                     batch_idx * batch_size, self.valid_set_size, 100. * batch_idx * batch_size / self.valid_set_size, \n",
        "#                     loss_sum.data.item(),\n",
        "#                     correct / batch_size,\n",
        "#                     list(map(lambda x: round(x, 2), selected_exp_cpu.tolist()[:4])),\n",
        "#                     list(map(lambda x: round(x, 2), pred_cpu.tolist()[:4])),\n",
        "#                     target_cpu.tolist()[:4],\n",
        "#                     round(time.time() - train_start_time, 3)\n",
        "#                     )\n",
        "#                 )\n",
        "#             else:\n",
        "#                 pass\n",
        "\n",
        "#         if purpose == 'valid':\n",
        "#             test_loss /= num_rows\n",
        "#             auc = np.mean(auc_list)\n",
        "#             num_correct = np.sum(correct_list)\n",
        "#             acc = (100. * num_correct) / num_rows\n",
        "#             # print(f\"target is {np.array(target_total_list)}\")\n",
        "#             # print(f\"pred is {np.array(pred_total_list)}\")\n",
        "#             sens = precision_score(np.array(target_total_list), np.array(pred_total_list))\n",
        "#             cm = confusion_matrix(np.array(target_total_list), np.array(pred_total_list))\n",
        "#             tn = cm[0, 0]\n",
        "#             tp = cm[1, 1]\n",
        "#             fn = cm[1, 0]\n",
        "#             fp = cm[0, 1]\n",
        "#             spec = tn / (tn + fp)\n",
        "#             prec = precision_score(np.array(target_total_list), np.array(pred_total_list))\n",
        "\n",
        "#             # fpr = fp/(fp + tn)\n",
        "#             # tpr = sens\n",
        "\n",
        "#             # roc_auc = auc(fpr, tpr)\n",
        "#             # plt.figure()\n",
        "#             # lw = 2\n",
        "#             # plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % auc)\n",
        "#             # plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "#             # plt.xlim([0.0, 1.0])\n",
        "#             # plt.ylim([0.0, 1.05])\n",
        "#             # plt.xlabel('False Positive Rate')\n",
        "#             # plt.ylabel('True Positive Rate')\n",
        "#             # plt.title('Receiver operating characteristic example')\n",
        "#             # plt.legend(loc=\"lower right\")\n",
        "#             # plt.show()\n",
        "#             print(f'tn is {tn}, tp is {tp}, fn is {fn}, fp is {fp}')\n",
        "#             print(\"Validation: Average loss: {:.4f}, AUC: {:.3f}, Acc: {}/{} ({:.3f}%) Recall: {:.3f} Prec: {:.3f} Spec: {}\\n\".format(\n",
        "#                   test_loss, \n",
        "#                   auc, \n",
        "#                   num_correct, num_rows, \n",
        "#                   acc,\n",
        "#                   sens,\n",
        "#                   prec,\n",
        "#                   spec\n",
        "#                   ))\n",
        "#             # try:\n",
        "#             fpr, tpr, _ = roc_curve(np.array(target_total_list), np.array(pred_total_list))\n",
        "#             # print(f\"Roc calculation result: fpr is {fpr}, tpr is {tpr}, {type(fpr)}, {type(tpr)}\")\n",
        "#             # roc_fpr[:, epoch] = fpr\n",
        "#             # roc_tpr[:, epoch] = tpr\n",
        "#             roc_auc = auc_area_calc(fpr, tpr)\n",
        "#             plt.figure()\n",
        "#             lw = 2\n",
        "#             plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "#             plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "#             plt.xlim([0.0, 1.0])\n",
        "#             plt.ylim([0.0, 1.05])\n",
        "#             plt.xlabel('False Positive Rate')\n",
        "#             plt.ylabel('True Positive Rate')\n",
        "#             plt.title('Receiver operating characteristic example')\n",
        "#             plt.legend(loc=\"lower right\")\n",
        "#             plt.show()\n",
        "#             # except TypeError:\n",
        "#             #     pass\n",
        "#             acc /= 100\n",
        "#             return test_loss.cpu().detach().numpy(), auc, acc, sens, prec\n",
        "\n",
        "#         if purpose == 'infer':\n",
        "#             return infer_total_list\n",
        "        \n",
        "#         if purpose == 'train':\n",
        "#             val_loss, val_auc, val_acc, val_sens, val_prec = self.validate()\n",
        "#             with self.writer.as_default():\n",
        "#                 loss_tensorboard /= num_rows\n",
        "#                 error = 1 - (acc_tensorboard / num_rows)\n",
        "#                 print(f\"summary val acc is {val_acc}\")\n",
        "#                 tf.summary.scalar('Loss/Train', loss_tensorboard, step=epoch)\n",
        "#                 tf.summary.scalar('Error/Train', error, step=epoch)\n",
        "#                 tf.summary.scalar('Loss/Val', val_loss, step=epoch)\n",
        "#                 tf.summary.scalar('Error/Val', (1 - val_acc), step=epoch)\n",
        "\n",
        "#             if val_acc > self.best_acc:\n",
        "#                 print(f\"acc update {val_acc}\")\n",
        "#                 self.best_acc = val_acc\n",
        "#                 self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "#             # print(\"{} secs elapsed.\".format(time.time() - init_time))\n",
        "\n",
        "#             # val_auc, val_acc, val_sens, val_prec = self.validate()\n",
        "\n",
        "#             # if self.save_path is not None:\n",
        "#             #     if isinstance(self.model, nn.DataParallel):\n",
        "#             #         model = self.model.module\n",
        "#             #     else:\n",
        "#             #         model = self.model\n",
        "                \n",
        "#             #     model_cpu = model.cpu()\n",
        "\n",
        "#             #     torch.save(model_cpu.state_dict(), self.save_path + f'{epoch}_{batch_idx}_{str(val_auc)[:5]}_{str(val_acc)[:5]}_{str(val_sens)[:5]}_{str(val_prec)[:5]}.pth')\n",
        "\n",
        "#             #     if torch.cuda.is_available():\n",
        "#             #         self.model = self.model.cuda()\n",
        "\n",
        "# # (self, data_dict_path, criterion, save_path=None, pretrained=False, bag_size=16, batch_size=1, valid_ratio=0.01, gpu_num=1, worker_ratio=4, save_interval=1000)\n",
        "\n",
        "# #     max_max = cMIL(DATA_DICT_PATH, criterion=CRITERION, save_path=SAVE_PATH, pretrained=True, bag_size=BAG_SIZE, \n",
        "# #                    batch_size=BATCH_SIZE, gpu_num=GPU_NUM, worker_ratio=WORKER_RATIO, valid_ratio=VALID_RATIO,\n",
        "# #                    save_interval=SAVE_INTERVAL)\n",
        "\n",
        "# # (self, criterion, train_loader, val_loader, full_loader, save_path=None, pretrained=False, bag_size=16, batch_size=1, worker_ratio=4, save_interval=1000)\n",
        "\n",
        "# #     max_max = cMIL(criterion=CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, bag_size=BAG_SIZE, \n",
        "# #                 batch_size=BATCH_SIZE, save_interval=SAVE_INTERVAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGxN4InoeTHb"
      },
      "source": [
        "# current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "# print(current_time)\n",
        "# log_dir = 'logs/learning_rate/' + current_time +'/Camel/max-max/lr=5e-8'\n",
        "# summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "# EPOCHS = 2\n",
        "# LR = 5e-8\n",
        "# SAVE_INTERVAL = 2\n",
        "# CRITERION = 'max-max'\n",
        "# SAVE_PATH = None\n",
        "    \n",
        "# max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "# best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# # max_max.load_model(best_model_wts)\n",
        "# # max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3EmH7FXuIXa"
      },
      "source": [
        "# Max-Max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFk3lC2qzd7G"
      },
      "source": [
        "## Learning rate determination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkDfhz9y85Q-"
      },
      "source": [
        "hyper_parameters = {\n",
        "    # Training Control Parameters\n",
        "    \n",
        "    # Dataset Parameters\n",
        "    'max_bag_size': 100,\n",
        "    'dataset_max_size': None,\n",
        "    'with_data_augmentation': False,\n",
        "    # 'with_tensorboard': not args.no_tensorboard,\n",
        "    'seed': 123,\n",
        "    'val_size': 0.15,\n",
        "    'test_size': 0,\n",
        "}\n",
        "\n",
        "logger = None\n",
        "input_width = 224\n",
        "train_dataset, val_dataset, test_dataset, whole_cases_ids, whole_indexes, whole_dataset = build_datasets(source_slides_folders=slides_folders,\n",
        "                                                              model_input_width=input_width,\n",
        "                                                              hyper_parameters=hyper_parameters,\n",
        "                                                              logger=logger)\n",
        "N_PROCESSES = 5\n",
        "def to_dataloader(dataset, for_training):\n",
        "    assert isinstance(dataset, Dataset) or isinstance(dataset, torch.utils.data.Subset)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=for_training, num_workers=N_PROCESSES)\n",
        "\n",
        "train_dataloader = to_dataloader(train_dataset, True)\n",
        "val_dataloader = to_dataloader(val_dataset, False) if len(val_dataset) else None\n",
        "test_dataloader = to_dataloader(test_dataset, False) if len(test_dataset) else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo_3lJMnWYSz"
      },
      "source": [
        "train_carcinoma = 0\n",
        "train_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(train_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      train_carcinoma += 1\n",
        "    else:\n",
        "      train_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the training set\" %(train_carcinoma, train_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")\n",
        "\n",
        "val_carcinoma = 0\n",
        "val_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(val_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      val_carcinoma += 1\n",
        "    else:\n",
        "      val_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the validation set\" %(val_carcinoma, val_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")\n",
        "\n",
        "test_carcinoma = 0\n",
        "test_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(test_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      test_carcinoma += 1\n",
        "    else:\n",
        "      test_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the test set\" %(test_carcinoma, test_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip0LPk-cbY9g"
      },
      "source": [
        "# for batch_idx, (data, bag_label, tiles_ids, slide_summary, _) in enumerate(val_dataloader):\n",
        "#   # print(len(tiles_ids))\n",
        "#   print(f\"There are {len(tiles_ids)} instances in the {batch_idx}-th bag\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Eg9AvSXVHwq"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/' + current_time +'/Camel/max-max/lr=5e-8'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 5e-8\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fY_FY_1RP9A6"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/' + current_time +'/Camel/max-max/lr=5e-7'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 5e-7\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-DmBu7sL5LA"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/' + current_time +'/Camel/max-max/lr=5e-6'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 5e-6\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87p5Pa-nZZ9t"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/' + current_time +'/Camel/max-max/lr=5e-5'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 5e-5\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHREf2bjys3C"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/' + current_time +'/Camel/max-max/lr=5e-4'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 5e-4\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urIkC4IUZc1W"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/' + current_time +'/Camel/max-max/lr=5e-3'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 5e-3\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnpQ5RDBLsLG"
      },
      "source": [
        "!rm -r logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y13xco2EEnoh"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir logs/learning_rate/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsTzAm7Zk_i"
      },
      "source": [
        "## Full Epochs Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZltsYdfZn_V"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/Camel/max-max/full_train/' + current_time +'/lr=5e-4'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 70\n",
        "LR = 5e-4\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ol1Nnm0xbNi"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir logs/Camel/max-max/full_train/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0ZqwTjx8IfS"
      },
      "source": [
        "## Test and Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKWlGAlbWRSM"
      },
      "source": [
        "hyper_parameters = {\n",
        "    # Training Control Parameters\n",
        "    \n",
        "    # Dataset Parameters\n",
        "    'max_bag_size': 100,\n",
        "    'dataset_max_size': None,\n",
        "    'with_data_augmentation': False,\n",
        "    # 'with_tensorboard': not args.no_tensorboard,\n",
        "    'seed': 123,\n",
        "    'val_size': 0.02,\n",
        "    'test_size': 0.95,\n",
        "}\n",
        "\n",
        "logger = None\n",
        "input_width = 224\n",
        "train_dataset, val_dataset, test_dataset, whole_cases_ids, whole_indexes, whole_dataset = build_datasets(source_slides_folders=slides_folders,\n",
        "                                                              model_input_width=input_width,\n",
        "                                                              hyper_parameters=hyper_parameters,\n",
        "                                                              logger=logger)\n",
        "N_PROCESSES = 5\n",
        "def to_dataloader(dataset, for_training):\n",
        "    assert isinstance(dataset, Dataset) or isinstance(dataset, torch.utils.data.Subset)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=for_training, num_workers=N_PROCESSES)\n",
        "\n",
        "train_dataloader = to_dataloader(train_dataset, True)\n",
        "val_dataloader = to_dataloader(val_dataset, False) if len(val_dataset) else None\n",
        "test_dataloader = to_dataloader(test_dataset, False) if len(test_dataset) else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV_S6vhH9Hmc"
      },
      "source": [
        "train_carcinoma = 0\n",
        "train_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(train_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      train_carcinoma += 1\n",
        "    else:\n",
        "      train_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the training set\" %(train_carcinoma, train_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")\n",
        "\n",
        "val_carcinoma = 0\n",
        "val_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(val_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      val_carcinoma += 1\n",
        "    else:\n",
        "      val_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the validation set\" %(val_carcinoma, val_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")\n",
        "\n",
        "test_carcinoma = 0\n",
        "test_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(test_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      test_carcinoma += 1\n",
        "    else:\n",
        "      test_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the test set\" %(test_carcinoma, test_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdYJ1lWLWPF4"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/' + current_time +'/Camel/max-max/lr=5e-8'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 5e-8\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "\n",
        "# To test on the test set, we replace the val_dataloader with test_dataloader and validate directly\n",
        "cMIL_maxmax_test = cMIL(summary_writer, CRITERION, None, test_dataloader, None, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "cMIL_maxmax_test.load_model(best_model_wts)\n",
        "val_loss, val_auc, val_acc, val_sens, val_prec = cMIL_maxmax_test.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXij1mJY2EAI"
      },
      "source": [
        "# Inference\n",
        "cMIL_maxmax_infer = cMIL(summary_writer, CRITERION, None, None, None, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "cMIL_maxmax_infer.load_model(best_model_wts)\n",
        "infer_total_list = cMIL_maxmax_infer.infer(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC1tiNn9uNIf"
      },
      "source": [
        "# Max-Min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAYwROJI2WoC"
      },
      "source": [
        "## Learning rate determination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_2ZBkWm2ZSx"
      },
      "source": [
        "hyper_parameters = {\n",
        "    # Training Control Parameters\n",
        "    \n",
        "    # Dataset Parameters\n",
        "    'max_bag_size': 100,\n",
        "    'dataset_max_size': None,\n",
        "    'with_data_augmentation': False,\n",
        "    # 'with_tensorboard': not args.no_tensorboard,\n",
        "    'seed': 123,\n",
        "    'val_size': 0.3,\n",
        "    'test_size': 0,\n",
        "}\n",
        "\n",
        "logger = None\n",
        "input_width = 224\n",
        "train_dataset, val_dataset, test_dataset, whole_cases_ids, whole_indexes, whole_dataset = build_datasets(source_slides_folders=slides_folders,\n",
        "                                                              model_input_width=input_width,\n",
        "                                                              hyper_parameters=hyper_parameters,\n",
        "                                                              logger=logger)\n",
        "N_PROCESSES = 5\n",
        "def to_dataloader(dataset, for_training):\n",
        "    assert isinstance(dataset, Dataset) or isinstance(dataset, torch.utils.data.Subset)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=for_training, num_workers=N_PROCESSES)\n",
        "\n",
        "train_dataloader = to_dataloader(train_dataset, True)\n",
        "val_dataloader = to_dataloader(val_dataset, False) if len(val_dataset) else None\n",
        "test_dataloader = to_dataloader(test_dataset, False) if len(test_dataset) else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiAGx0Nq2aQj"
      },
      "source": [
        "train_carcinoma = 0\n",
        "train_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(train_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      train_carcinoma += 1\n",
        "    else:\n",
        "      train_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the training set\" %(train_carcinoma, train_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")\n",
        "\n",
        "val_carcinoma = 0\n",
        "val_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(val_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      val_carcinoma += 1\n",
        "    else:\n",
        "      val_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the validation set\" %(val_carcinoma, val_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")\n",
        "\n",
        "test_carcinoma = 0\n",
        "test_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(test_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      test_carcinoma += 1\n",
        "    else:\n",
        "      test_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the test set\" %(test_carcinoma, test_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P25YdQhM2hjg"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/Camel/max-min/' + current_time +'/lr=5e-8'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 30\n",
        "LR = 5e-8\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-min'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC4JhFqc3RmV"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/Camel/max-min/' + current_time +'/lr=5e-7'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 30\n",
        "LR = 5e-7\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-min'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkrK97uQ3UKc"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/Camel/max-min/' + current_time +'/lr=5e-5'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 30\n",
        "LR = 5e-5\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-min'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UqIYGYU3WLj"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/Camel/max-min/' + current_time +'/lr=5e-4'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 30\n",
        "LR = 5e-4\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-min'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb9cX7Y33YCn"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/learning_rate/Camel/max-min/' + current_time +'/lr=5e-3'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 30\n",
        "LR = 5e-3\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-min'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jnsdjmw4QiN"
      },
      "source": [
        "!rm -r logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOlRMlP04X3w"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir logs/learning_rate/Camel/max-min/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlY2xjtZ2mI2"
      },
      "source": [
        "## Full Epochs train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDsuNvTU2q4s"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/Camel/max-min/' + current_time +'/lr=5e-4'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 70\n",
        "LR = 5e-4\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-min'\n",
        "SAVE_PATH = None\n",
        "    \n",
        "max_max = cMIL(summary_writer, CRITERION, train_dataloader, val_dataloader, test_dataloader, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "best_model_wts = max_max.train(EPOCHS, lr=LR)\n",
        "# max_max.load_model(best_model_wts)\n",
        "# max.max.validate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "947TN0pQPpny"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir logs/Camel/max-min/0516-2213/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCrOcKw13g15"
      },
      "source": [
        "## Test and Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOGHd9qN3iSg"
      },
      "source": [
        "hyper_parameters = {\n",
        "    # Training Control Parameters\n",
        "    \n",
        "    # Dataset Parameters\n",
        "    'max_bag_size': 100,\n",
        "    'dataset_max_size': None,\n",
        "    'with_data_augmentation': False,\n",
        "    # 'with_tensorboard': not args.no_tensorboard,\n",
        "    'seed': 123,\n",
        "    'val_size': 0.02,\n",
        "    'test_size': 0.95,\n",
        "}\n",
        "\n",
        "logger = None\n",
        "input_width = 224\n",
        "train_dataset, val_dataset, test_dataset, whole_cases_ids, whole_indexes, whole_dataset = build_datasets(source_slides_folders=slides_folders,\n",
        "                                                              model_input_width=input_width,\n",
        "                                                              hyper_parameters=hyper_parameters,\n",
        "                                                              logger=logger)\n",
        "N_PROCESSES = 5\n",
        "def to_dataloader(dataset, for_training):\n",
        "    assert isinstance(dataset, Dataset) or isinstance(dataset, torch.utils.data.Subset)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=for_training, num_workers=N_PROCESSES)\n",
        "\n",
        "train_dataloader = to_dataloader(train_dataset, True)\n",
        "val_dataloader = to_dataloader(val_dataset, False) if len(val_dataset) else None\n",
        "test_dataloader = to_dataloader(test_dataset, False) if len(test_dataset) else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSFz4EZW3ksW"
      },
      "source": [
        "train_carcinoma = 0\n",
        "train_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(train_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      train_carcinoma += 1\n",
        "    else:\n",
        "      train_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the training set\" %(train_carcinoma, train_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")\n",
        "\n",
        "val_carcinoma = 0\n",
        "val_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(val_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      val_carcinoma += 1\n",
        "    else:\n",
        "      val_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the validation set\" %(val_carcinoma, val_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")\n",
        "\n",
        "test_carcinoma = 0\n",
        "test_non_carcinoma = 0\n",
        "try:\n",
        "  for batch_idx, (data, bag_label, _, _, _) in enumerate(test_dataloader):\n",
        "    if bag_label == torch.Tensor([1]):\n",
        "      test_carcinoma += 1\n",
        "    else:\n",
        "      test_non_carcinoma += 1\n",
        "  print(\"There are %d carcinoma and %d non-carcinoma samples in the test set\" %(test_carcinoma, test_non_carcinoma))\n",
        "except TypeError:\n",
        "  print(\"Nan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjDI63r13unQ"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "print(current_time)\n",
        "log_dir = 'logs/Camel/max-min/test/' + current_time +'/lr=5e-7'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 5e-7\n",
        "SAVE_INTERVAL = 2\n",
        "CRITERION = 'max-max'\n",
        "SAVE_PATH = None\n",
        "\n",
        "# To test on the test set, we replace the val_dataloader with test_dataloader and validate directly\n",
        "cMIL_maxmax_test = cMIL(summary_writer, CRITERION, None, test_dataloader, None, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "cMIL_maxmax_test.load_model(best_model_wts)\n",
        "val_loss, val_auc, val_acc, val_sens, val_prec = cMIL_maxmax_test.validate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9oQ1qXK3wpV"
      },
      "source": [
        "# Inference\n",
        "cMIL_maxmax_infer = cMIL(summary_writer, CRITERION, None, None, None, save_path=SAVE_PATH, pretrained=True, save_interval=SAVE_INTERVAL)\n",
        "cMIL_maxmax_infer.load_model(best_model_wts)\n",
        "infer_total_list = cMIL_maxmax_infer.infer(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ISihTwQseZ"
      },
      "source": [
        "# Infer_maxmax"
      ]
    }
  ]
}